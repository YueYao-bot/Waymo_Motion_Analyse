{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82bfcc9a-43f3-4f5d-92d7-40ef4928b48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "\n",
    "import warnings\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecae62c-55d8-4dc8-9a3c-bac55c71a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f6aab0-a05b-4c4f-a8ab-ed3f1c47b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfp.__version__, tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55239ba0-0fd3-4975-a0c0-3109bb961dc6",
   "metadata": {},
   "source": [
    "# Utils Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eef6ba-0721-49af-842f-432c2845cbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, np.float32):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.int16):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.int64):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, tf.Tensor):\n",
    "            return float(obj)\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b493cb-db2d-4561-aec9-46df1d0e044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_result(degree, bic_scores, aic_scores, A_list, B_list, losses, best_epochs, lr, optimizer, epochs, batch_size):\n",
    "    # BIC Results\n",
    "    best_bic_deg_idx = np.where(bic_scores == np.amin(bic_scores))[0][0]\n",
    "    best_bic_deg = degrees[best_bic_deg_idx]\n",
    "    bic_best_A = A_list[best_bic_deg_idx]\n",
    "    bic_best_B = B_list[best_bic_deg_idx]\n",
    "    best_bic = bic_scores[best_bic_deg_idx]\n",
    "    \n",
    "    # AIC Results\n",
    "    best_aic_deg_idx = np.where(aic_scores == np.amin(aic_scores))[0][0]\n",
    "    best_aic_deg = degrees[best_aic_deg_idx]\n",
    "    aic_best_A = A_list[best_aic_deg_idx]\n",
    "    aic_best_B = B_list[best_aic_deg_idx]\n",
    "    best_aic = aic_scores[best_aic_deg_idx]\n",
    "    \n",
    "    last_losses = [loss[-1] for loss in losses]\n",
    "    \n",
    "    result = {'degree': degree,\n",
    "          'losses': last_losses,\n",
    "          'A_list': A_list,\n",
    "          'B_list': B_list,\n",
    "              \n",
    "          'bic_scores': bic_scores,\n",
    "          'best_bic': best_bic,\n",
    "          'best_bic_A': bic_best_A,\n",
    "          'best_bic_B': bic_best_B,\n",
    "          'best_bic_deg': best_bic_deg,\n",
    "          'best_bic_deg_idx': best_bic_deg_idx,\n",
    "            \n",
    "          'aic_scores': aic_scores,\n",
    "          'best_aic': best_aic,\n",
    "          'best_aic_A': aic_best_A,\n",
    "          'best_aic_B': aic_best_B,\n",
    "          'best_aic_deg': best_aic_deg,\n",
    "          'best_aic_deg_idx': best_aic_deg_idx,\n",
    "              \n",
    "          'lr': lr,\n",
    "          'optimizer': optimizer,\n",
    "          'epochs': epochs,\n",
    "          'best_epochs': best_epochs,\n",
    "          'batch_size': batch_size\n",
    "        }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12352b59-5c93-4554-a467-95e179e34246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(folder_dir, file_name, result):  \n",
    "    with open(folder_dir + '/' + file_name + '.json', \"w\") as write_file:\n",
    "        json.dump(result, write_file, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee701645-55be-4bcc-9dc2-94e46377a609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(dist, samples):\n",
    "    \"\"\"Calculates the negative log-likelihood for a given distribution\n",
    "    and a data set.\"\"\"\n",
    "    ll = dist.log_prob(samples)\n",
    "    mask_ll = tf.boolean_mask(ll, tf.math.is_finite(ll))\n",
    "    ll = tf.where(tf.math.is_finite(ll), ll, [-1000])\n",
    "    if mask_ll.shape[0] / ll.shape[0] < 0.7:\n",
    "        print('Too much nan in one batch', mask_ll.shape[0], ll.shape[0] )\n",
    "    return -tf.reduce_mean(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8400541-c5c6-43a1-a8a7-bbe6cbc2fe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def get_loss_and_grads(dist, samples):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(dist.trainable_variables)\n",
    "        loss = nll(dist, samples)\n",
    "    grads = tape.gradient(loss, dist.trainable_variables)\n",
    "\n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65094ded-6d93-4d5b-b9c8-0f59cb3b03c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_distribution(dist, samples, opti, epoch):\n",
    "    loss, grads = get_loss_and_grads(dist, samples)\n",
    "\n",
    "    if tf.math.is_finite(loss) and tf.math.is_finite(grads[1]):\n",
    "        opti.apply_gradients(zip(grads, dist.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af793ceb-24d7-4b88-a0fe-80867186be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expands a vector to a polynomial design matrix: from a constant to the deg-power\n",
    "def polyBasisScale(x_last, deg):\n",
    "    #Expands a vector to a polynomial design matrix: from a constant to the deg-power\n",
    "    return np.diag(np.squeeze((np.column_stack([x_last**deg for deg in range(0, deg+1)]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f71c293-eb4c-4545-8c94-b4b056a269ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_basis_function(x, power):\n",
    "    return x ** power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5906a16-ee63-44d0-8049-b3a7bece7632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(x, bf, bf_args=None):\n",
    "    if bf_args is None:\n",
    "        return np.concatenate([np.ones(x.shape), bf(x)], axis=1)\n",
    "    else:\n",
    "        return np.array([np.ones(x.shape)] + [bf(x, bf_arg) for bf_arg in bf_args]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8160fdd-639c-41f4-bbae-bcd818af64a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(losses, degrees, y_lim = (-50,50)):\n",
    "    fig, ax = plt.subplots()\n",
    "    for i,loss in enumerate(losses):\n",
    "        ax.plot(range(len(loss)), loss, label = str(degrees[i]))\n",
    "    \n",
    "    ax.set_ylim(y_lim)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1884e91f-fdc6-4e9c-9b10-88f665effb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_file_list_dataset(path_list, idx_trajs_select):\n",
    "    path_list_select = []\n",
    "\n",
    "    for root, dirs, files in os.walk(os.path.abspath(path_list)):\n",
    "        files.sort()\n",
    "        for idx, file in enumerate(tqdm(files)):\n",
    "            if idx in idx_trajs_select:\n",
    "                path_list_select.append(os.path.join(root, file))\n",
    "\n",
    "    file_list_dataset = tf.data.Dataset.from_tensor_slices(path_list_select)\n",
    "    \n",
    "    return file_list_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4e1764-1d04-4cee-93f5-c2c1d3d9aaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_start_indicies_dataset(start_indicies_file):\n",
    "    with open(start_indicies_file, \"r\") as read_file:\n",
    "        start_indicies_all = np.array((json.load(read_file)))\n",
    "\n",
    "    start_indicies_dataset = tf.data.Dataset.from_tensor_slices(start_indicies_all)\n",
    "    \n",
    "    return start_indicies_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56360ee-783c-448b-b3e7-ccbed6a45a3c",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77da203-f8b6-4d76-b15d-0455313c7c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    def __init__(self, batch_size, dataset, num_points_in_one_traj):\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset = dataset  \n",
    "        self.num_points_in_one_traj = num_points_in_one_traj\n",
    "        self.loaded_dataset = None\n",
    "    \n",
    "    def _extract_ego_trajs(self, file_path, start_idx):\n",
    "        file_str = str(file_path.numpy())[2:-1]\n",
    "        ego_trajs_all = []\n",
    "        times_all = []\n",
    "        with open(file_str, \"r\") as read_file:\n",
    "            traj_data = json.load(read_file)\n",
    "            \n",
    "        ego_traj_temp = np.array(traj_data['ego_traj'])[start_idx : start_idx+self.num_points_in_one_traj]\n",
    "        agt_traj_temp = np.array(traj_data['agt_trajs'])[start_idx : start_idx+self.num_points_in_one_traj]\n",
    "        \n",
    "        d = agt_traj_temp[:, [0,1]] - ego_traj_temp[:, [0,1]] \n",
    "        \n",
    "        c_map_to_ego, s_map_to_ego = np.cos(-ego_traj_temp[:, 3]), -np.sin(ego_traj_temp[:, 3])\n",
    "        R_map_to_ego = np.transpose(np.array(((c_map_to_ego, -s_map_to_ego), (s_map_to_ego, c_map_to_ego))), (2, 0, 1))\n",
    "\n",
    "        c_ego_to_map, s_ego_to_map = np.cos(ego_traj_temp[:, 3]), np.sin(ego_traj_temp[:, 3])\n",
    "        R_ego_to_map = np.transpose(np.array(((c_ego_to_map, -s_ego_to_map), (s_ego_to_map, c_ego_to_map))), (2, 0, 1))\n",
    "        \n",
    "        d_rotated = np.abs(np.squeeze(R_map_to_ego  @ d[:, :, None]))\n",
    "    \n",
    "        ego_traj_temp = ego_traj_temp[:, :2] - ego_traj_temp[0,:2] # let ego trajectories start from zero\n",
    "        ego_traj = np.concatenate((ego_traj_temp[:, 0], ego_traj_temp[:, 1]), axis = 0)\n",
    "        \n",
    "        agt_traj_temp = agt_traj_temp[:, :2] - agt_traj_temp[0,:2] # let agt trajectories start from zero\n",
    "        agt_traj = np.concatenate((agt_traj_temp[:, 0], agt_traj_temp[:, 1]), axis = 0)\n",
    "        \n",
    "        times = np.array(traj_data['timestamp'])[start_idx: start_idx+self.num_points_in_one_traj]\n",
    "        times = times - times[0]\n",
    "        \n",
    "        times = tf.convert_to_tensor(times, dtype=tf.float32)\n",
    "        ego_traj = tf.convert_to_tensor(ego_traj, dtype=tf.float32)\n",
    "        agt_traj = tf.convert_to_tensor(agt_traj, dtype=tf.float32)\n",
    "        d_rotated = tf.convert_to_tensor(d_rotated, dtype=tf.float32)\n",
    "        R_ego_to_map = tf.convert_to_tensor(R_ego_to_map, dtype=tf.float32)\n",
    "        \n",
    "        return times, ego_traj, agt_traj, d_rotated, R_ego_to_map\n",
    "    \n",
    "    \n",
    "    def _load_data(self, file_path, start_idx):\n",
    "        return tf.py_function(self._extract_ego_trajs, [file_path, start_idx], [tf.float32, tf.float32, tf.float32, tf.float32, tf.float32])\n",
    "    \n",
    "    def load_process(self, shuffle = False):\n",
    "        self.loaded_dataset = self.dataset.map(map_func = self._load_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        self.loaded_dataset = self.loaded_dataset.cache()\n",
    "\n",
    "        # Shuffle data and create batches\n",
    "        if shuffle:\n",
    "            self.loaded_dataset = self.loaded_dataset.shuffle(buffer_size=self.loaded_dataset.__len__())\n",
    "        \n",
    "        # Set batch size for dataset\n",
    "        self.loaded_dataset = self.loaded_dataset.batch(self.batch_size)\n",
    "\n",
    "        # Make dataset fetch batches in the background during the training of the model.\n",
    "        self.loaded_dataset = self.loaded_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "    def get_batch(self):\n",
    "        return next(iter(self.loaded_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64bc30a-2f10-4940-8e12-ea523e23aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 100\n",
    "lr = 1e-4\n",
    "\n",
    "original_num_points_in_one_traj = 91\n",
    "num_points_in_one_traj = 91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a25dd9b-d3c9-4289-9e43-5d86ceab412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/agt_trajs_\" + str(num_points_in_one_traj) + \"_json/agt_trajs_select_non_outlier_indicies.json\", \"r\") as read_file:\n",
    "    idx_trajs_select = json.load(read_file)\n",
    "    \n",
    "#list_dataset = tf.data.Dataset.list_files(str('data/ego_trajs_json/*'), shuffle=False)\n",
    "list_dataset = generate_file_list_dataset('data/agt_trajs_json/', idx_trajs_select)\n",
    "start_idx_dataset = generate_start_indicies_dataset(\"data/agt_trajs_\" + str(num_points_in_one_traj) + \"_json/agt_trajs_start_point_indicies.json\")\n",
    "combined_dataset = tf.data.Dataset.zip((list_dataset, start_idx_dataset))\n",
    "\n",
    "dataProcessor = DataProcessor(BATCH_SIZE, combined_dataset, num_points_in_one_traj)\n",
    "dataProcessor.load_process(shuffle = False)\n",
    "\n",
    "print(dataProcessor.loaded_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4663178-fdaf-4e1f-9cff-e610010e54a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(idx_invalid_idx), len(idx_not_moving), len(idx_outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14683580-50ef-4f95-b9f1-dd2c91e96b1b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962cf4ab-30db-4c9c-a22e-ec8f86916cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mvn(alpha, beta_lon, beta_lat, phi_t, phi_d_lon, phi_d_lat, R, num_points):\n",
    "    def mvn_from_alpha_beta(alpha, beta_lon, beta_lat, phi_t, phi_d_lon, phi_d_lat, R):      \n",
    "        b_lon = tf.squeeze(phi_d_lon @ tf.math.softplus(beta_lon))\n",
    "        b_lat = tf.squeeze(phi_d_lat @ tf.math.softplus(beta_lat))\n",
    "\n",
    "        b_diag = tf.linalg.diag(tf.concat([b_lon, b_lat], axis = 1))\n",
    "        cov = R @ b_diag @ tf.transpose(R, perm=[0, 2, 1]) + (phi_t @ alpha )  @ (tf.transpose(phi_t @ alpha, perm=[0, 2, 1]))\n",
    "\n",
    "        return tfd.MultivariateNormalFullCovariance(loc=tf.zeros((2*num_points)), scale_tril = tf.linalg.cholesky(cov))\n",
    "\n",
    "    return tfp.experimental.util.DeferredModule(build_fn=mvn_from_alpha_beta, alpha=alpha, beta_lon=beta_lon, beta_lat=beta_lat, \n",
    "                                                phi_t = phi_t, phi_d_lon=phi_d_lon, phi_d_lat=phi_d_lat, R=R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04802dc7-1a69-4c28-8e28-b4a2fa4eb1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_AIC_BIC(nll, deg, num_points):\n",
    "    # Compute bayesian information criterion\n",
    "    degree_of_freedom = 6 + (2*deg)*(2*(deg)+1) / 2\n",
    "    bic_score = nll + 0.5 * np.log(num_points) * degree_of_freedom\n",
    "    \n",
    "    # Compute Akaike information criterion\n",
    "    aic_score = nll + degree_of_freedom\n",
    "    \n",
    "    return aic_score, bic_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fe0d9d-119c-4439-a238-5ffd136804bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(alpha, beta_lon, beta_lat, opti, data_loader, epochs = 100, tf_summary_writer = None, verbose = False, early_stop = True):\n",
    "    model_losses = []\n",
    "    best_alpha = None\n",
    "    best_beta_diag, best_beta_by_diag = None, None\n",
    "    best_epoch_loss = np.inf\n",
    "    best_epoch = 0\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        batch_losses = []\n",
    "        for timestamp_samples, trajectories_samples in data_loader:\n",
    "            phi_t_batch = expand(((t_samples)/t_scale_factor), bf=polynomial_basis_function, bf_args=range(1, deg+1)).transpose((1, 0, 2))\n",
    "            phi_t_kron = np.kron(np.eye(2), phi_t_batch[:, :, 1:])\n",
    "            \n",
    "            phi_d_lon = expand(d_samples[:, :, 0], bf=polynomial_basis_function, bf_args=range(1, 2+1)).transpose((1, 0, 2))\n",
    "            phi_d_lat = expand(d_samples[:, :, 1], bf=polynomial_basis_function, bf_args=range(1, 2+1)).transpose((1, 0, 2))\n",
    "\n",
    "            R = np.block([[tf.linalg.diag(R_samples[:, 0, 0]), tf.linalg.diag(R_samples[:, 0, 1])],\n",
    "                          [tf.linalg.diag(R_samples[:, 1, 0]), tf.linalg.diag(R_samples[:, 1, 1])]])\n",
    "            \n",
    "            mvn_test = build_mvn(alpha=alpha, beta_lon=beta_lon, beta_lat=beta_lat, phi_t=phi_t_kron, \n",
    "                                 phi_d_lon=phi_d_lon, phi_d_lat=phi_d_lat, R=R, num_points=num_points_in_one_traj)\n",
    "\n",
    "            batch_loss = fit_distribution(mvn_test, trajectories_samples, optimizer,epoch)\n",
    "            batch_losses.append(batch_loss)\n",
    "            \n",
    "            tf.keras.backend.clear_session() # clear the initiated model in this loop\n",
    "        gc.collect()\n",
    "            \n",
    "        assert not tf.math.is_nan(np.mean(batch_losses))\n",
    "        \n",
    "        epoch_loss = np.mean(batch_losses)\n",
    "        \n",
    "        if epoch_loss < best_epoch_loss:\n",
    "            best_epoch_loss = epoch_loss\n",
    "            best_epoch = epoch\n",
    "            best_alpha, best_beta_diag, best_beta_by_diag = deepcopy(alpha), deepcopy(beta_diag), deepcopy(beta_by_diag)\n",
    "        \n",
    "        model_losses.append(epoch_loss)\n",
    "        \n",
    "        if tf_summary_writer:\n",
    "            with tf_summary_writer.as_default():\n",
    "                tf.summary.scalar('loss', np.mean(batch_losses), step=epoch)\n",
    "        \n",
    "        # Early stop if epoch loss doesn't decrease for more then 20 epochs \n",
    "        if early_stop and epoch - best_epoch >=20:\n",
    "            print('Early Stop at ' + str(epoch) + '(' + str(best_epoch) + ')' + ' epoch')\n",
    "            break\n",
    "        \n",
    "        if(epoch %10 == 0 and verbose):\n",
    "        #    A_scale_mat = polyBasisScale(t_scale_factor, deg)\n",
    "        #    A_scale_mat = A_scale_mat[1:, 1:]\n",
    "        #    A_est = np.linalg.inv(np.kron(np.eye(2), A_scale_mat)) @ A.numpy()\n",
    "        #    A_est = A_est @ A_est.T\n",
    "            print('Epoch ', epoch, ', Loss: ', model_losses[-1])\n",
    "        #    print(tf.math.softplus(B_diag), tf.math.softplus(B_diag) * tf.math.tanh(B_by_diag))\n",
    "        #    print(np.diag(A_est))\n",
    "        #    #print('Rank: ', np.linalg.matrix_rank(mvn_test.covariance()))\n",
    "        \n",
    "    return model_losses, best_epoch_loss, best_epoch, best_alpha, best_beta_lon, best_beta_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec295bb5-cd46-4032-9e15-55fdbc6b53d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "best_losses = []\n",
    "best_epoch_losses = []\n",
    "best_epochs = []\n",
    "bic_scores = []\n",
    "aic_scores = []\n",
    "A_list, B_list = [], []\n",
    "lr_schedules_ser = []\n",
    "optimizers_ser = []\n",
    "log_root_dir = 'logs/gradient_tape/agt_xy' + str(traj_len) + ''\n",
    "t_scale_factor = (num_points_in_one_traj-1) / 10 # The time duration of one trajectory\n",
    "nan_batches = []\n",
    "degrees = np.linspace(1, 8, 8, dtype=np.int16) # analyse polynomial from degree 1 to 8\n",
    "#degrees = [3,4,5]\n",
    "for i_d, deg in enumerate(degrees):\n",
    "    print('Trainig deg ',deg)\n",
    "    model_losses = []\n",
    "    boundaries = [dataset.__len__().numpy()*100, dataset.__len__().numpy()*150]\n",
    "    #values = [0.001, 0.0005, 1e-4]\n",
    "    values = [0.005, 0.001, 5e-4]\n",
    "    lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)   \n",
    "    \n",
    "    lr_schedules_ser.append(tf.keras.optimizers.schedules.serialize(lr_schedule))\n",
    "    optimizers_ser.append(tf.keras.optimizers.serialize(optimizer))\n",
    "\n",
    "    A = tf.Variable(np.random.randn(2*(deg), 2*(deg))* 1e-1, dtype=tf.float32, name='alpha') # Model uncertainty\n",
    "    B_lon = tf.Variable(np.random.randn(3, 1), dtype=tf.float32, name='beta_lon') \n",
    "    B_lat =  tf.Variable(np.random.randn(3, 1), dtype=tf.float32, name='beta_lat')\n",
    "    \n",
    "    train_log_dir = log_root_dir + '/deg_' + str(deg)\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)  \n",
    "\n",
    "    model_losses, best_epoch_loss, best_epoch, best_alpha, best_beta_lon, best_beta_lat = train(alpha=A, beta_lon=B_lon, beta_lat=B_lat, opti=optimizer, \n",
    "                                                     epochs = EPOCHS, data_loader=dataProcessor.loaded_dataset, tf_summary_writer = train_summary_writer, verbose = False, early_stop=False)\n",
    "            \n",
    "    # Add loss\n",
    "    losses.append(model_losses)\n",
    "    best_epoch_losses.append([best_epoch_loss])\n",
    "    \n",
    "    # store the best epoch\n",
    "    best_epochs.append(best_epoch)\n",
    "    \n",
    "    # Compute AIC and BIC\n",
    "    aic_score, bic_score = compute_AIC_BIC(nll = best_epoch_loss, deg = deg, num_points = num_points_in_one_traj)\n",
    "\n",
    "    bic_scores.append(bic_score)\n",
    "    aic_scores.append(aic_score)\n",
    "    \n",
    "    # Compute the model uncertainty, A_unscaled = np.linalg.inv(scale_mat) @ A_scaled\n",
    "    A_scale_mat = polyBasisScale(t_scale_factor, deg)\n",
    "    A_scale_mat = A_scale_mat[1:, 1:]\n",
    "    A_est = np.linalg.inv(np.kron(np.eye(2), A_scale_mat)) @ A.numpy()\n",
    "    A_est = A_est @ A_est.T\n",
    "    A_list.append(A_est)\n",
    "    \n",
    "    # Compute the observation uncertainty, B_cov = tf.eye(num_points_in_one_traj) * tf.math.softplus(B)\n",
    "    B_est = {'B_lon': (tf.math.softplus(B_lon)).numpy(), 'B_lat': (tf.math.softplus(B_lat)).numpy()}\n",
    "    B_list.append(B_est)\n",
    "    print(deg, model_losses[-1], bic_score, aic_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p38)",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
