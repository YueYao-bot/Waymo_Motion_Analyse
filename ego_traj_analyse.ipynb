{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f037793-2e5e-4d37-ac00-d63c5df532c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "\n",
    "import utils\n",
    "import dataloader_utils\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gc\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "tfd = tfp.distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "768871e7-16c0-4d02-a470-fc590a8e8ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-24 20:30:19.904154: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a779bdfc-44ae-4753-a1c0-86e1862e2cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0.16.0', '2.8.0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfp.__version__, tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b0d26d-5c2f-438f-8b11-c7a8df8c901c",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94f63899-b64e-4e33-ab55-e5230eef71a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "intercept = False # we recommand to set this to false to ignore the 0th order of polynomial\n",
    "num_points_in_one_traj = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f4895b5-ec71-43df-8c65-547514ac8841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 487002/487002 [00:00<00:00, 1032517.97it/s]\n",
      "100%|██████████| 487002/487002 [00:00<00:00, 3023500.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(377, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/ego_trajs_not_moving_indicies.json\", \"r\") as read_file:\n",
    "    idx_not_moving = set(json.load(read_file))\n",
    "    \n",
    "with open(\"data/ego_trajs_\" + str(num_points_in_one_traj) + \"_json/ego_trajs_outlier_indicies.json\", \"r\") as read_file:\n",
    "    idx_outlier = set(json.load(read_file))\n",
    "\n",
    "idx_invalid_idx = idx_outlier | idx_not_moving\n",
    "    \n",
    "list_dataset = dataloader_utils.generate_ego_file_list_dataset('data/ego_trajs_json/', idx_invalid_idx)\n",
    "start_idx_dataset = dataloader_utils.generate_ego_start_indicies_dataset(\"data/ego_trajs_\" + str(num_points_in_one_traj) + \"_json/ego_trajs_start_point_indicies.json\", idx_invalid_idx)\n",
    "combined_dataset = tf.data.Dataset.zip((list_dataset, start_idx_dataset))\n",
    "\n",
    "dataProcessor = dataloader_utils.DataProcessor(BATCH_SIZE, combined_dataset, num_points_in_one_traj, traj_type = 'ego_traj', intercept = intercept)\n",
    "dataProcessor.load_process(shuffle = True)\n",
    "\n",
    "print(dataProcessor.loaded_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acc55f77-59b0-4d7c-bdf9-fbd42b7d7e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101105, 101096, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_invalid_idx), len(idx_not_moving), len(idx_outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8131d465-809e-4a9c-b18d-2b712bcae77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 11) (1024, 22)\n"
     ]
    }
   ],
   "source": [
    "# Check if the output dimensions are correct\n",
    "# timestamp has dim [batch_size, num_points_in_one_traj]\n",
    "# trajectories has dim [batch_size, 2*num_points_in_one_traj]\n",
    "for timestamp_samples, trajectories_samples in dataProcessor.loaded_dataset:\n",
    "    print(timestamp_samples.shape, trajectories_samples.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8155b5ec-ebc4-4ca4-a8fb-5f91c1b7650c",
   "metadata": {},
   "source": [
    "# Analyse ego_xy with observation noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4adaafb-5fa2-491f-8983-e1356d806a5c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b7afe70-4857-485f-8605-366c862d0945",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "lr = 5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76a9634b-ef15-48aa-8688-cd60844c0c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mvn(alpha, beta_diag, beta_by_diag, phi_t, num_points):\n",
    "    '''\n",
    "    build multivariate normal distribution for trajectory batch\n",
    "    Inputs:\n",
    "        alpha: trainable variable for model parameter covariance\n",
    "        beta_diag: trainable variable for diagonal entities of observation covariance\n",
    "        beta_by_diag: trainable variable for non-diagonal entities of observation covariance\n",
    "        phi_t: batch-wise basis function\n",
    "        num_points: number of sample points in one trajectory\n",
    "    Outputs:\n",
    "        tf distribution\n",
    "    '''\n",
    "    def mvn_from_alpha_beta(alpha, beta_diag, beta_by_diag, phi_t):      \n",
    "        b_by_diag = tf.eye(num_points_in_one_traj, dtype = tf.float64) * tf.math.softplus(beta_diag) * tf.math.tanh(beta_by_diag)\n",
    "        by_eye = tf.convert_to_tensor([[0,1],[1,0]], dtype=tf.float64)\n",
    "        b_diag = tf.eye(2*num_points, dtype=tf.float64) * tf.math.softplus(beta_diag)\n",
    "        b_kron = b_diag  + tf.experimental.numpy.kron(by_eye, b_by_diag)\n",
    "\n",
    "        cov =   b_kron + (phi_t @ alpha )  @ (tf.transpose(phi_t @ alpha, perm=[0, 2, 1]))\n",
    "        \n",
    "        return tfd.MultivariateNormalTriL(loc=tf.zeros((2* num_points), dtype = tf.float64), scale_tril=tf.linalg.cholesky(cov))\n",
    "    \n",
    "    return tfp.experimental.util.DeferredModule(build_fn=mvn_from_alpha_beta, alpha=alpha, beta_diag=beta_diag, beta_by_diag=beta_by_diag, phi_t = phi_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de8aaca8-93ba-488b-947e-1033959dd6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(alpha, beta_diag, beta_by_diag, opti, data_loader, epochs = 100, tf_summary_writer = None, verbose = False, early_stop = True):\n",
    "    model_losses = []\n",
    "    best_alpha = None\n",
    "    best_beta_diag, best_beta_by_diag = None, None\n",
    "    best_epoch_loss = np.inf\n",
    "    best_epoch = 0\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        batch_losses = []\n",
    "        for timestamp_samples, trajectories_samples in data_loader:\n",
    "\n",
    "            phi_t_batch = utils.expand(timestamp_samples/t_scale_factor, bf=utils.polynomial_basis_function, bf_args=range(1, deg+1)).transpose((1, 0, 2))\n",
    "            \n",
    "            if intercept:\n",
    "                phi_t_kron = np.kron(np.eye(2), phi_t_batch)\n",
    "            else:\n",
    "                phi_t_kron = np.kron(np.eye(2), phi_t_batch[:, :, 1:])\n",
    "            \n",
    "            # Cast to float64 for better numerical stability\n",
    "            phi_t_kron = tf.cast(phi_t_kron, dtype = tf.float64)\n",
    "            trajectories_samples = tf.cast(trajectories_samples, dtype = tf.float64)\n",
    "            \n",
    "            # build multivariate normal distribution with learnable variables.\n",
    "            mvn_test = build_mvn(alpha=alpha, beta_diag=beta_diag, beta_by_diag=beta_by_diag, phi_t = phi_t_kron, num_points = num_points_in_one_traj)\n",
    "\n",
    "            batch_loss = utils.fit_distribution(mvn_test, trajectories_samples, optimizer,epoch)\n",
    "            batch_losses.append(batch_loss)\n",
    "            \n",
    "            tf.keras.backend.clear_session() # clear the initiated model in this loop\n",
    "        gc.collect()\n",
    "            \n",
    "        assert not tf.math.is_nan(np.mean(batch_losses))\n",
    "        \n",
    "        epoch_loss = np.mean(batch_losses)\n",
    "        \n",
    "        if epoch_loss < best_epoch_loss:\n",
    "            best_epoch_loss = epoch_loss\n",
    "            best_epoch = epoch\n",
    "            best_alpha, best_beta_diag, best_beta_by_diag = deepcopy(alpha), deepcopy(beta_diag), deepcopy(beta_by_diag)\n",
    "        \n",
    "        model_losses.append(epoch_loss)\n",
    "        \n",
    "        if tf_summary_writer:\n",
    "            with tf_summary_writer.as_default():\n",
    "                tf.summary.scalar('loss', np.mean(batch_losses), step=epoch)\n",
    "        \n",
    "        # Early stop if epoch loss doesn't decrease for more then 20 epochs \n",
    "        if early_stop and epoch - best_epoch >=20:\n",
    "            print('Early Stop at ' + str(epoch) + '(' + str(best_epoch) + ')' + ' epoch')\n",
    "            break\n",
    "        \n",
    "        if(epoch %10 == 0 and verbose):\n",
    "        #    A_scale_mat = polyBasisScale(t_scale_factor, deg)\n",
    "        #    A_scale_mat = A_scale_mat[1:, 1:]\n",
    "        #    A_est = np.linalg.inv(np.kron(np.eye(2), A_scale_mat)) @ A.numpy()\n",
    "        #    A_est = A_est @ A_est.T\n",
    "            print('Epoch ', epoch, ', Loss: ', model_losses[-1])\n",
    "        #    print(tf.math.softplus(B_diag), tf.math.softplus(B_diag) * tf.math.tanh(B_by_diag))\n",
    "        #    print(np.diag(A_est))\n",
    "        #    #print('Rank: ', np.linalg.matrix_rank(mvn_test.covariance()))\n",
    "        \n",
    "    return model_losses, best_epoch_loss, best_epoch, best_alpha, best_beta_diag, best_beta_by_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf66ef63-d97e-406f-bcb7-75266f80fea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig deg  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:41<00:00, 41.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , Loss:  93.93805752331583\n",
      "1 93.93805752331583 99.93279570531176 98.93805752331583\n",
      "Trainig deg  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:41<00:00, 41.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , Loss:  41.659279838531134\n",
      "2 41.659279838531134 56.046651475321354 53.659279838531134\n",
      "Trainig deg  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:41<00:00, 41.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , Loss:  37.03853655132661\n",
      "3 37.03853655132661 64.61433218850787 60.03853655132661\n",
      "Trainig deg  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:41<00:00, 41.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , Loss:  31.562846578584125\n",
      "4 31.562846578584125 77.12285676175317 69.56284657858413\n",
      "Trainig deg  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:22<00:00, 82.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , Loss:  22.931544735362813\n",
      "5 22.931544735362813 91.27156001011639 79.93154473536282\n",
      "Trainig deg  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:22<00:00, 82.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , Loss:  22.449081776489574\n",
      "6 22.449081776489574 118.3648926884244 102.44908177648958\n",
      "Trainig deg  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:22<00:00, 82.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , Loss:  28.663529925286316\n",
      "7 28.663529925286316 156.95092701999917 135.66352992528633\n",
      "Trainig deg  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:45<00:00, 45.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , Loss:  16.309780725474692\n",
      "8 16.309780725474692 181.76455454856227 154.3097807254747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "best_epoch_losses = []\n",
    "best_epochs = []\n",
    "bic_scores = []\n",
    "aic_scores = []\n",
    "A_list, B_list = [], []\n",
    "lr_schedules_ser = []\n",
    "optimizers_ser = []\n",
    "log_root_dir = 'logs/gradient_tape/ego_xy' + str(num_points_in_one_traj) + ''\n",
    "t_scale_factor = (num_points_in_one_traj-1) / 10 # The time duration of one trajectory\n",
    "nan_batches = []\n",
    "degrees = np.linspace(1, 8, 8, dtype=np.int16) # analyse polynomials from degree 1 to 8\n",
    "#degrees = [2]\n",
    "for i_d, deg in enumerate(degrees):\n",
    "    print('Trainig deg ',deg)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)   \n",
    "    optimizers_ser.append(tf.keras.optimizers.serialize(optimizer))\n",
    "    \n",
    "    if intercept:\n",
    "        A = tf.Variable(np.random.randn(2*(deg+1), 2*(deg+1)) * 1e-1 , dtype=tf.float64, name='alpha') # Model uncertainty\n",
    "    else:\n",
    "        A = tf.Variable(np.random.randn(2*(deg), 2*(deg)) * 1e-1 , dtype=tf.float64, name='alpha') # Model uncertainty\n",
    "        \n",
    "    B_diag = tf.Variable(np.random.randn(1) * 1e-1, dtype=tf.float64, name='beta_diag') # Log of Observation uncertainty\n",
    "    B_by_diag =  tf.Variable(np.random.randn(1) * 1e-1, dtype=tf.float64, name='beta_by_diag')\n",
    "\n",
    "    \n",
    "    train_log_dir = log_root_dir + '/deg_' + str(deg)\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)  \n",
    "   \n",
    "    model_losses, best_epoch_loss, best_epoch, best_alpha, best_beta_diag, best_beta_by_diag = train(alpha=A, beta_diag=B_diag, beta_by_diag=B_by_diag, opti=optimizer, \n",
    "                                                     epochs = EPOCHS, data_loader=dataProcessor.loaded_dataset, tf_summary_writer = train_summary_writer, verbose = True, early_stop=False)\n",
    "            \n",
    "    # Add model loss\n",
    "    losses.append(model_losses)\n",
    "    best_epoch_losses.append([best_epoch_loss])\n",
    "    \n",
    "    # store the best epoch\n",
    "    best_epochs.append(best_epoch)\n",
    "    \n",
    "    # Compute the AIC and BIC score\n",
    "    aic_score, bic_score = utils.compute_AIC_BIC(nll = best_epoch_loss, deg = deg, num_points = num_points_in_one_traj)\n",
    "\n",
    "    bic_scores.append(bic_score)\n",
    "    aic_scores.append(aic_score)\n",
    "    \n",
    "    # Compute the model uncertainty, A_unscaled = np.linalg.inv(scale_mat) @ A_scaled\n",
    "    A_scale_mat = utils.polyBasisScale(t_scale_factor, deg)\n",
    "    if not intercept:\n",
    "        A_scale_mat = A_scale_mat[1:, 1:]\n",
    "    A_est = np.linalg.inv(np.kron(np.eye(2), A_scale_mat)) @ best_alpha.numpy()\n",
    "    A_est = A_est @ A_est.T\n",
    "    A_list.append(A_est)\n",
    "    \n",
    "    # Compute the observation uncertainty, B_cov = tf.eye(num_points_in_one_traj) * tf.math.softplus(B)\n",
    "    B_est = {'B_diag': (tf.math.softplus(best_beta_diag)).numpy(), \n",
    "             'B_by_diag': (tf.math.softplus(best_beta_diag) * tf.math.tanh(best_beta_by_diag)).numpy()}\n",
    "    B_list.append(B_est)\n",
    "    print(deg, model_losses[-1], bic_score, aic_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd7a60c6-bbe8-4b9d-bdc3-9c9cad925f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = utils.calculate_result(degrees, bic_scores, aic_scores, A_list, B_list, best_epoch_losses, best_epochs, lr, None, EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19bd5c-7b07-4cfd-acce-aa87a0ba29db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = utils.plot_losses(losses, degrees = degrees, y_lim=[-400, 100])\n",
    "if intercept:\n",
    "    fig.savefig('imgs/ego_' + str(num_points_in_one_traj) + '_intercept.svg')\n",
    "else:\n",
    "    fig.savefig('imgs/ego_' + str(num_points_in_one_traj) + '_new.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce08d82a-9157-4610-9542-3d4ec62435ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.14522466e+01, -2.73942286e-01, -4.10133558e-02,\n",
       "         5.76802892e-04,  2.60812928e-04,  1.00650900e-08,\n",
       "        -1.10050386e-06, -1.51519316e+00,  3.36503576e-02,\n",
       "         5.09021338e-04, -5.24760282e-04,  2.66848690e-05,\n",
       "         1.53142460e-06, -1.08119344e-07],\n",
       "       [-2.73942286e-01,  1.99848203e-01, -4.33435507e-02,\n",
       "         6.73612048e-03, -4.71637003e-04, -2.96163099e-06,\n",
       "         1.31961829e-06, -4.55225715e-02,  8.14936940e-04,\n",
       "         1.78261024e-03, -2.12300858e-04,  1.11887377e-05,\n",
       "        -1.18823473e-06,  8.35346261e-08],\n",
       "       [-4.10133558e-02, -4.33435507e-02,  2.83199940e-02,\n",
       "        -6.12331564e-03,  5.51016605e-04, -1.25400125e-05,\n",
       "        -5.27205369e-07, -1.24517638e-03, -2.91873655e-03,\n",
       "         6.79339345e-04, -8.18916865e-05,  3.86088155e-06,\n",
       "         2.49962880e-07, -2.63205034e-08],\n",
       "       [ 5.76802892e-04,  6.73612048e-03, -6.12331564e-03,\n",
       "         1.77385703e-03, -2.27150042e-04,  1.26769281e-05,\n",
       "        -2.19576213e-07,  3.95464843e-04,  6.68537662e-04,\n",
       "        -4.09747847e-04,  9.54299016e-05, -1.06340892e-05,\n",
       "         5.46732473e-07, -9.23382338e-09],\n",
       "       [ 2.60812928e-04, -4.71637003e-04,  5.51016605e-04,\n",
       "        -2.27150042e-04,  4.17567436e-05, -3.57616383e-06,\n",
       "         1.16831163e-07, -1.16228937e-06, -8.46698991e-05,\n",
       "         7.47539965e-05, -2.36096898e-05,  3.41823648e-06,\n",
       "        -2.31620808e-07,  5.85452663e-09],\n",
       "       [ 1.00650900e-08, -2.96163099e-06, -1.25400125e-05,\n",
       "         1.26769281e-05, -3.57616383e-06,  4.05712957e-07,\n",
       "        -1.63841536e-08, -3.19329944e-06,  6.13281019e-06,\n",
       "        -6.08558046e-06,  2.38080376e-06, -4.08230020e-07,\n",
       "         3.17551646e-08, -9.11830740e-10],\n",
       "       [-1.10050386e-06,  1.31961829e-06, -5.27205369e-07,\n",
       "        -2.19576213e-07,  1.16831163e-07, -1.63841536e-08,\n",
       "         7.42070843e-10,  1.74949857e-07, -1.98367452e-07,\n",
       "         1.90787221e-07, -8.76410590e-08,  1.69771779e-08,\n",
       "        -1.44584910e-09,  4.47611984e-11],\n",
       "       [-1.51519316e+00, -4.55225715e-02, -1.24517638e-03,\n",
       "         3.95464843e-04, -1.16228937e-06, -3.19329944e-06,\n",
       "         1.74949857e-07,  4.01367958e+01, -3.75695617e-01,\n",
       "        -4.84388887e-02,  1.41223810e-03,  1.75118812e-04,\n",
       "         1.07990988e-05, -1.57328586e-06],\n",
       "       [ 3.36503576e-02,  8.14936940e-04, -2.91873655e-03,\n",
       "         6.68537662e-04, -8.46698991e-05,  6.13281019e-06,\n",
       "        -1.98367452e-07, -3.75695617e-01,  2.24377951e-01,\n",
       "        -4.90114570e-02,  7.96924482e-03, -6.14847798e-04,\n",
       "         3.87246096e-06,  1.25432195e-06],\n",
       "       [ 5.09021338e-04,  1.78261024e-03,  6.79339345e-04,\n",
       "        -4.09747847e-04,  7.47539965e-05, -6.08558046e-06,\n",
       "         1.90787221e-07, -4.84388887e-02, -4.90114570e-02,\n",
       "         3.24419792e-02, -7.20142639e-03,  6.83245367e-04,\n",
       "        -1.97340810e-05, -4.05213737e-07],\n",
       "       [-5.24760282e-04, -2.12300858e-04, -8.18916865e-05,\n",
       "         9.54299016e-05, -2.36096898e-05,  2.38080376e-06,\n",
       "        -8.76410590e-08,  1.41223810e-03,  7.96924482e-03,\n",
       "        -7.20142639e-03,  2.09628737e-03, -2.71266792e-04,\n",
       "         1.54384013e-05, -2.81315477e-07],\n",
       "       [ 2.66848690e-05,  1.11887377e-05,  3.86088155e-06,\n",
       "        -1.06340892e-05,  3.41823648e-06, -4.08230020e-07,\n",
       "         1.69771779e-08,  1.75118812e-04, -6.14847798e-04,\n",
       "         6.83245367e-04, -2.71266792e-04,  4.85994543e-05,\n",
       "        -4.07810600e-06,  1.30926388e-07],\n",
       "       [ 1.53142460e-06, -1.18823473e-06,  2.49962880e-07,\n",
       "         5.46732473e-07, -2.31620808e-07,  3.17551646e-08,\n",
       "        -1.44584910e-09,  1.07990988e-05,  3.87246096e-06,\n",
       "        -1.97340810e-05,  1.54384013e-05, -4.07810600e-06,\n",
       "         4.49396984e-07, -1.78583662e-08],\n",
       "       [-1.08119344e-07,  8.35346261e-08, -2.63205034e-08,\n",
       "        -9.23382338e-09,  5.85452663e-09, -9.11830740e-10,\n",
       "         4.47611984e-11, -1.57328586e-06,  1.25432195e-06,\n",
       "        -4.05213737e-07, -2.81315477e-07,  1.30926388e-07,\n",
       "        -1.78583662e-08,  7.99823855e-10]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['best_aic_A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cabc5eb2-b979-452a-8ac2-7535785138be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B_diag': array([0.00026802]), 'B_by_diag': array([-3.16223622e-06])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['best_aic_B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d68f64f-e682-4c66-aa5b-dfdbc0336d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_result(folder_dir =log_root_dir, file_name='result_summary', result=result)\n",
    "with open(log_root_dir + '/' + 'optimizers' + '.json', \"w\") as write_file:\n",
    "    json.dump(optimizers_ser, write_file, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e19940e-7ff0-482e-91b0-8db194375700",
   "metadata": {},
   "source": [
    "# Dummy Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81580e57-143c-4507-887c-996a2135eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points_in_one_traj = 21\n",
    "deg_to_change = 1\n",
    "idx_to_change = deg_to_change-1\n",
    "with open('logs/gradient_tape/ego_xy' + str(num_points_in_one_traj) + '/result_summary.json', \"r\") as read_file:\n",
    "    result_old = json.load(read_file)\n",
    "          \n",
    "with open('logs/gradient_tape/ego_xy' + str(num_points_in_one_traj) + '_only_' + str(deg_to_change) + 'th/result_summary.json', \"r\") as read_file:\n",
    "    result_sub = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44a52a28-134d-4f3d-a5b7-342db327d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_old['losses'][idx_to_change] = result_sub['losses'][0]\n",
    "result_old['A_list'][idx_to_change] = result_sub['A_list'][0]\n",
    "result_old['B_list'][idx_to_change] = result_sub['B_list'][0]\n",
    "\n",
    "result_old['lr'][idx_to_change] = result_sub['lr'][0]\n",
    "result_old['optimizer'][idx_to_change] = result_sub['optimizer'][0]\n",
    "\n",
    "result_old['bic_scores'][idx_to_change] = result_sub['bic_scores'][0]\n",
    "best_bic_deg_idx = np.where(result_old['bic_scores'] == np.amin(result_old['bic_scores']))[0][0]\n",
    "result_old['best_bic'] = result_old['bic_scores'][best_bic_deg_idx]\n",
    "result_old['best_bic_A'] = result_old['A_list'][best_bic_deg_idx]\n",
    "result_old['best_bic_B'] = result_old['B_list'][best_bic_deg_idx]\n",
    "result_old['best_bic_deg'] = result_old['degree'][best_bic_deg_idx]\n",
    "result_old['best_bic_deg_idx'] = best_bic_deg_idx\n",
    "\n",
    "\n",
    "result_old['aic_scores'][idx_to_change] = result_sub['aic_scores'][0]\n",
    "best_aic_deg_idx = np.where(result_old['aic_scores'] == np.amin(result_old['aic_scores']))[0][0]\n",
    "result_old['best_aic'] = result_old['aic_scores'][best_aic_deg_idx]\n",
    "result_old['best_aic_A'] = result_old['A_list'][best_aic_deg_idx]\n",
    "result_old['best_aic_B'] = result_old['B_list'][best_aic_deg_idx]\n",
    "result_old['best_aic_deg'] = result_old['degree'][best_aic_deg_idx]\n",
    "result_old['best_aic_deg_idx'] = best_aic_deg_idx\n",
    "\n",
    "with open('logs/gradient_tape/ego_xy' + str(num_points_in_one_traj) + '/result_summary_new.json', \"w\") as write_file:\n",
    "    json.dump(result_old, write_file, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d21d813-c785-408a-8248-547388ff3e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p38)",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
