{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f037793-2e5e-4d37-ac00-d63c5df532c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "\n",
    "import utils.utils as utils\n",
    "import utils.dataloader_utils as dataloader_utils\n",
    "import utils.train_utils as train_utils\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "768871e7-16c0-4d02-a470-fc590a8e8ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-24 20:30:19.904154: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a779bdfc-44ae-4753-a1c0-86e1862e2cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0.16.0', '2.8.0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfp.__version__, tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b0d26d-5c2f-438f-8b11-c7a8df8c901c",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94f63899-b64e-4e33-ab55-e5230eef71a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "intercept = False # we recommand to set this to false to ignore the 0th order of polynomial\n",
    "num_points_in_one_traj = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f4895b5-ec71-43df-8c65-547514ac8841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 487002/487002 [00:00<00:00, 1032517.97it/s]\n",
      "100%|██████████| 487002/487002 [00:00<00:00, 3023500.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(377, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/ego_trajs_not_moving_indicies.json\", \"r\") as read_file:\n",
    "    idx_not_moving = set(json.load(read_file))\n",
    "    \n",
    "with open(\"data/ego_trajs_\" + str(num_points_in_one_traj) + \"_json/ego_trajs_outlier_indicies.json\", \"r\") as read_file:\n",
    "    idx_outlier = set(json.load(read_file))\n",
    "\n",
    "idx_invalid_idx = idx_outlier | idx_not_moving\n",
    "    \n",
    "list_dataset = dataloader_utils.generate_file_list_dataset('data/ego_trajs_json/', idx_invalid_idx)\n",
    "start_idx_dataset = dataloader_utils.generate_start_indicies_dataset(\"data/ego_trajs_\" + str(num_points_in_one_traj) + \"_json/ego_trajs_start_point_indicies.json\", idx_invalid_idx)\n",
    "combined_dataset = tf.data.Dataset.zip((list_dataset, start_idx_dataset))\n",
    "\n",
    "dataProcessor = dataloader_utils.DataProcessor(BATCH_SIZE, combined_dataset, num_points_in_one_traj, traj_type = 'ego_traj', intercept = intercept)\n",
    "dataProcessor.load_process(shuffle = True)\n",
    "\n",
    "print(dataProcessor.loaded_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acc55f77-59b0-4d7c-bdf9-fbd42b7d7e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101105, 101096, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_invalid_idx), len(idx_not_moving), len(idx_outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8131d465-809e-4a9c-b18d-2b712bcae77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 11) (1024, 22)\n"
     ]
    }
   ],
   "source": [
    "# Check if the output dimensions are correct\n",
    "# timestamp has dim [batch_size, num_points_in_one_traj]\n",
    "# trajectories has dim [batch_size, 2*num_points_in_one_traj]\n",
    "for timestamp_samples, trajectories_samples in dataProcessor.loaded_dataset:\n",
    "    print(timestamp_samples.shape, trajectories_samples.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8155b5ec-ebc4-4ca4-a8fb-5f91c1b7650c",
   "metadata": {},
   "source": [
    "# Analyse ego_xy with observation noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4adaafb-5fa2-491f-8983-e1356d806a5c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b7afe70-4857-485f-8605-366c862d0945",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "lr = 5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf66ef63-d97e-406f-bcb7-75266f80fea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig deg  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:41<00:00, 41.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , Loss:  93.93805752331583\n",
      "1 93.93805752331583 99.93279570531176 98.93805752331583\n",
      "Trainig deg  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:41<00:00, 41.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , Loss:  41.659279838531134\n",
      "2 41.659279838531134 56.046651475321354 53.659279838531134\n",
      "Trainig deg  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:41<00:00, 41.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , Loss:  37.03853655132661\n",
      "3 37.03853655132661 64.61433218850787 60.03853655132661\n",
      "Trainig deg  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:41<00:00, 41.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , Loss:  31.562846578584125\n",
      "4 31.562846578584125 77.12285676175317 69.56284657858413\n",
      "Trainig deg  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:22<00:00, 82.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , Loss:  22.931544735362813\n",
      "5 22.931544735362813 91.27156001011639 79.93154473536282\n",
      "Trainig deg  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:22<00:00, 82.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , Loss:  22.449081776489574\n",
      "6 22.449081776489574 118.3648926884244 102.44908177648958\n",
      "Trainig deg  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:22<00:00, 82.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , Loss:  28.663529925286316\n",
      "7 28.663529925286316 156.95092701999917 135.66352992528633\n",
      "Trainig deg  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:45<00:00, 45.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , Loss:  16.309780725474692\n",
      "8 16.309780725474692 181.76455454856227 154.3097807254747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "best_epoch_losses = []\n",
    "best_epochs = []\n",
    "bic_scores = []\n",
    "aic_scores = []\n",
    "A_list, B_list = [], []\n",
    "lr_schedules_ser = []\n",
    "optimizers_ser = []\n",
    "log_root_dir = 'logs/gradient_tape/ego_xy' + str(num_points_in_one_traj) + ''\n",
    "t_scale_factor = (num_points_in_one_traj-1) / 10 # The time duration of one trajectory, for scaling time to interval (0,1)\n",
    "nan_batches = []\n",
    "degrees = np.linspace(1, 8, 8, dtype=np.int16) # analyse polynomials from degree 1 to 8\n",
    "#degrees = [2]\n",
    "for i_d, deg in enumerate(degrees):\n",
    "    print('Trainig deg ',deg)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)   \n",
    "    optimizers_ser.append(tf.keras.optimizers.serialize(optimizer))\n",
    "    \n",
    "    if intercept:\n",
    "        A = tf.Variable(np.random.randn(2*(deg+1), 2*(deg+1)) * 1e-1 , dtype=tf.float64, name='alpha') # Model uncertainty\n",
    "    else:\n",
    "        A = tf.Variable(np.random.randn(2*(deg), 2*(deg)) * 1e-1 , dtype=tf.float64, name='alpha') # Model uncertainty\n",
    "        \n",
    "    B_diag = tf.Variable(np.random.randn(1) * 1e-1, dtype=tf.float64, name='beta_diag') # Log of Observation uncertainty\n",
    "    B_by_diag =  tf.Variable(np.random.randn(1) * 1e-1, dtype=tf.float64, name='beta_by_diag')\n",
    "\n",
    "    \n",
    "    train_log_dir = log_root_dir + '/deg_' + str(deg)\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)  \n",
    "   \n",
    "    model_losses, best_epoch_loss, best_epoch, best_alpha, best_beta_diag, best_beta_by_diag = train_utils.train_ego(alpha=A, beta_diag=B_diag, beta_by_diag=B_by_diag, t_scale_factor = t_scale_factor, degree = deg,\n",
    "                                                                                                                     opti=optimizer, epochs = EPOCHS, data_loader=dataProcessor.loaded_dataset, tf_summary_writer = train_summary_writer, \n",
    "                                                                                                                     verbose = True, early_stop=False)\n",
    "            \n",
    "    # Add model loss\n",
    "    losses.append(model_losses)\n",
    "    best_epoch_losses.append([best_epoch_loss])\n",
    "    \n",
    "    # store the best epoch\n",
    "    best_epochs.append(best_epoch)\n",
    "    \n",
    "    # Compute the AIC and BIC score\n",
    "    aic_score, bic_score = utils.compute_AIC_BIC(nll = best_epoch_loss, deg = deg, num_points = num_points_in_one_traj)\n",
    "\n",
    "    bic_scores.append(bic_score)\n",
    "    aic_scores.append(aic_score)\n",
    "    \n",
    "    # Compute the model uncertainty, A_unscaled = np.linalg.inv(scale_mat) @ A_scaled\n",
    "    A_scale_mat = utils.polyBasisScale(t_scale_factor, deg)\n",
    "    if not intercept:\n",
    "        A_scale_mat = A_scale_mat[1:, 1:]\n",
    "    A_est = np.linalg.inv(np.kron(np.eye(2), A_scale_mat)) @ best_alpha.numpy()\n",
    "    A_est = A_est @ A_est.T\n",
    "    A_list.append(A_est)\n",
    "    \n",
    "    # Compute the observation uncertainty, B_cov = tf.eye(num_points_in_one_traj) * tf.math.softplus(B)\n",
    "    B_est = {'B_diag': (tf.math.softplus(best_beta_diag)).numpy(), \n",
    "             'B_by_diag': (tf.math.softplus(best_beta_diag) * tf.math.tanh(best_beta_by_diag)).numpy()}\n",
    "    B_list.append(B_est)\n",
    "    print(deg, model_losses[-1], bic_score, aic_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd7a60c6-bbe8-4b9d-bdc3-9c9cad925f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = utils.calculate_result(degrees, bic_scores, aic_scores, A_list, B_list, best_epoch_losses, best_epochs, lr, None, EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19bd5c-7b07-4cfd-acce-aa87a0ba29db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = utils.plot_losses(losses, degrees = degrees, y_lim=[-400, 100])\n",
    "if intercept:\n",
    "    fig.savefig('imgs/ego_' + str(num_points_in_one_traj) + '_intercept.svg')\n",
    "else:\n",
    "    fig.savefig('imgs/ego_' + str(num_points_in_one_traj) + '_new.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cabc5eb2-b979-452a-8ac2-7535785138be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B_diag': array([0.00026802]), 'B_by_diag': array([-3.16223622e-06])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['best_aic_B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d68f64f-e682-4c66-aa5b-dfdbc0336d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_result(folder_dir =log_root_dir, file_name='result_summary', result=result)\n",
    "with open(log_root_dir + '/' + 'optimizers' + '.json', \"w\") as write_file:\n",
    "    json.dump(optimizers_ser, write_file, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e19940e-7ff0-482e-91b0-8db194375700",
   "metadata": {},
   "source": [
    "# Dummy Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81580e57-143c-4507-887c-996a2135eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points_in_one_traj = 21\n",
    "deg_to_change = 1\n",
    "idx_to_change = deg_to_change-1\n",
    "with open('logs/gradient_tape/ego_xy' + str(num_points_in_one_traj) + '/result_summary.json', \"r\") as read_file:\n",
    "    result_old = json.load(read_file)\n",
    "          \n",
    "with open('logs/gradient_tape/ego_xy' + str(num_points_in_one_traj) + '_only_' + str(deg_to_change) + 'th/result_summary.json', \"r\") as read_file:\n",
    "    result_sub = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44a52a28-134d-4f3d-a5b7-342db327d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_old['losses'][idx_to_change] = result_sub['losses'][0]\n",
    "result_old['A_list'][idx_to_change] = result_sub['A_list'][0]\n",
    "result_old['B_list'][idx_to_change] = result_sub['B_list'][0]\n",
    "\n",
    "result_old['lr'][idx_to_change] = result_sub['lr'][0]\n",
    "result_old['optimizer'][idx_to_change] = result_sub['optimizer'][0]\n",
    "\n",
    "result_old['bic_scores'][idx_to_change] = result_sub['bic_scores'][0]\n",
    "best_bic_deg_idx = np.where(result_old['bic_scores'] == np.amin(result_old['bic_scores']))[0][0]\n",
    "result_old['best_bic'] = result_old['bic_scores'][best_bic_deg_idx]\n",
    "result_old['best_bic_A'] = result_old['A_list'][best_bic_deg_idx]\n",
    "result_old['best_bic_B'] = result_old['B_list'][best_bic_deg_idx]\n",
    "result_old['best_bic_deg'] = result_old['degree'][best_bic_deg_idx]\n",
    "result_old['best_bic_deg_idx'] = best_bic_deg_idx\n",
    "\n",
    "\n",
    "result_old['aic_scores'][idx_to_change] = result_sub['aic_scores'][0]\n",
    "best_aic_deg_idx = np.where(result_old['aic_scores'] == np.amin(result_old['aic_scores']))[0][0]\n",
    "result_old['best_aic'] = result_old['aic_scores'][best_aic_deg_idx]\n",
    "result_old['best_aic_A'] = result_old['A_list'][best_aic_deg_idx]\n",
    "result_old['best_aic_B'] = result_old['B_list'][best_aic_deg_idx]\n",
    "result_old['best_aic_deg'] = result_old['degree'][best_aic_deg_idx]\n",
    "result_old['best_aic_deg_idx'] = best_aic_deg_idx\n",
    "\n",
    "with open('logs/gradient_tape/ego_xy' + str(num_points_in_one_traj) + '/result_summary_new.json', \"w\") as write_file:\n",
    "    json.dump(result_old, write_file, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d21d813-c785-408a-8248-547388ff3e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p38)",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
